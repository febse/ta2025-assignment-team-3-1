{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2c5807",
   "metadata": {},
   "source": [
    "# Introduction to Text Analytics Assignment\n",
    "\n",
    "The first code cell below loads a subset of 5000 movie reviews from the IMDB dataset. Each review is labeled as either positive or negative. The task here is to compare the performance of different text classification methods on this dataset.\n",
    "\n",
    "Train a classifier of your choice (e.g., logistic regression, SVM, decision tree) using only the review text to create features. Evaluate the classifier using accuracy, precision, recall, and F1-score when predicting the sentiment labels.\n",
    "\n",
    "- Use TF-IDF vectorization as one of the feature extraction methods with varying n-gram ranges (e.g., unigrams, bigrams).\n",
    "- Compare the results with using a lower-dimensional representation of the text data using Singular Value Decomposition (SVD) on the TF-IDF matrix.\n",
    "- Experiment with word embeddings (e.g., Word2Vec, GloVe) to represent the text data and train a classifier on these embeddings (by using the average of word vectors for each review to create a document-level representation).\n",
    "- Finally, use the document embeddings of two different LLMs available via OpenAI API (e.g., text-embedding-ada-002 and text-embedding-3-small) to represent the reviews and train classifiers on these embeddings.\n",
    "\n",
    "## Working on a Local Machine\n",
    "\n",
    "You can edit the notebook on Google Colab or locally on your computer. The project dependencies are managed by `uv`. For local development, download and install `uv` from [here](https://docs.astral.sh/uv/getting-started/installation/) then run the following command in your terminal to set up the environment:\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "This will create a virtual environment under `.venv` in the project directory and install all required dependencies. You can connect this environment to the Jupyter notebook by selecting the appropriate kernel (in VSCode, hit Ctrl+Shift+P and search for \"Python: Select Interpreter\").\n",
    "\n",
    "## Working on Google Colab\n",
    "\n",
    "You can download the notebook from the GitHub repository and upload it to Google Colab. When you work on it you can save intermediate results to your Google Drive (find the command in the File menu). When you are done, download the completed notebook and upload it to your GitHub repository.\n",
    "\n",
    "## Using OpenAI API\n",
    "\n",
    "To use the OpenAI API, get the API key from [here](https://firebasestorage.googleapis.com/v0/b/uni-sofia.appspot.com/o/lit%2Foc.txt?alt=media&token=768020c6-62d2-4c1b-9c53-966c322922e0) and edit the first code cell to set the API key in the `OpenAI` client as shown below:\n",
    "\n",
    "## How to submit\n",
    "\n",
    "When you are done with the assignment, please upload the completed notebook to your GitHub repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8060920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\krisi\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\krisi\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: openai in c:\\users\\krisi\\anaconda3\\lib\\site-packages (2.8.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\krisi\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really liked this Summerslam due to the look...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not many television shows appeal to quite as m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The film quickly gets to a major chase scene w...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane Austen would definitely approve of this o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expectations were somewhat high for me when I ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  I really liked this Summerslam due to the look...  positive\n",
       "1  Not many television shows appeal to quite as m...  positive\n",
       "2  The film quickly gets to a major chase scene w...  negative\n",
       "3  Jane Austen would definitely approve of this o...  positive\n",
       "4  Expectations were somewhat high for me when I ...  negative"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install gensim tqdm openai\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "import gensim.downloader as api\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(api_key=\"WRITE_THE_API_KEY_HERE\")\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/febse/data/raw/refs/heads/main/ta/IMDB-Dataset-5000.csv.zip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3d1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'word2vec_model' not in globals():\n",
    "    # Download the pretrained Word2Vec vectors (GoogleNews-vectors-negative300)\n",
    "    word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "if 'glove_model' not in globals():\n",
    "    # Download the pretrained GloVe vectors (glove-wiki-gigaword-300)\n",
    "    glove_model = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Get embedding for a text using OpenAI's API (v1.0+ compatible).\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The text to embed\n",
    "    model (str): The embedding model to use (default: \"text-embedding-3-small\")\n",
    "    \n",
    "    Returns:\n",
    "    list: The embedding vector\n",
    "    \"\"\"\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# get_openai_embedding(\"This is a sample text.\", model=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee5e68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1536\n",
      "[0.03873104974627495, 0.025061266496777534, 0.025133363902568817, 0.007267478853464127, -0.031694281846284866, -0.04994949698448181, -0.012177352793514729, -0.018384991213679314, 0.0011130128987133503, 0.004736838862299919]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: one embedding call\n",
    "test_vec = get_openai_embedding(\"This is a sample text.\", model=\"text-embedding-3-small\")\n",
    "print(type(test_vec), len(test_vec))\n",
    "print(test_vec[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0a8ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure numeric labels\n",
    "df[\"y\"] = df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1}).astype(int)\n",
    "\n",
    "X = df[\"review\"].astype(str).tolist()\n",
    "y = df[\"y\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "len(X_train), len(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e03295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_binary(y_true, y_pred, name=\"model\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    print(f\"{name} | acc={acc:.4f} prec={prec:.4f} rec={rec:.4f} f1={f1:.4f}\")\n",
    "    return {\"model\": name, \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c41b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_embeddings(texts, model=\"text-embedding-3-small\", batch_size=64):\n",
    "    vectors = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Embedding ({model})\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        resp = openai_client.embeddings.create(model=model, input=batch)\n",
    "        vectors.extend([d.embedding for d in resp.data])\n",
    "    return np.array(vectors, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40d26dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 1536), (1000, 1536))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(\"cache\", exist_ok=True)\n",
    "\n",
    "model_1 = \"text-embedding-3-small\"\n",
    "Xtr_path_1 = f\"cache/Xtr_{model_1}.npy\"\n",
    "Xte_path_1 = f\"cache/Xte_{model_1}.npy\"\n",
    "\n",
    "if os.path.exists(Xtr_path_1) and os.path.exists(Xte_path_1):\n",
    "    Xtr_1 = np.load(Xtr_path_1)\n",
    "    Xte_1 = np.load(Xte_path_1)\n",
    "else:\n",
    "    Xtr_1 = get_openai_embeddings(X_train, model=model_1, batch_size=64)\n",
    "    Xte_1 = get_openai_embeddings(X_test,  model=model_1, batch_size=64)\n",
    "    np.save(Xtr_path_1, Xtr_1)\n",
    "    np.save(Xte_path_1, Xte_1)\n",
    "\n",
    "Xtr_1.shape, Xte_1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0c3eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI text-embedding-3-small + LR | acc=0.9390 prec=0.9301 rec=0.9504 f1=0.9401\n"
     ]
    }
   ],
   "source": [
    "clf_1 = LogisticRegression(max_iter=3000)\n",
    "clf_1.fit(Xtr_1, y_train)\n",
    "pred_1 = clf_1.predict(Xte_1)\n",
    "\n",
    "results.append(eval_binary(y_test, pred_1, name=f\"OpenAI {model_1} + LR\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536a5834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 1536), (1000, 1536))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = \"text-embedding-ada-002\"   # assignment example\n",
    "# model_2 = \"text-embedding-3-large\" # alternative if allowed\n",
    "\n",
    "Xtr_path_2 = f\"cache/Xtr_{model_2}.npy\"\n",
    "Xte_path_2 = f\"cache/Xte_{model_2}.npy\"\n",
    "\n",
    "if os.path.exists(Xtr_path_2) and os.path.exists(Xte_path_2):\n",
    "    Xtr_2 = np.load(Xtr_path_2)\n",
    "    Xte_2 = np.load(Xte_path_2)\n",
    "else:\n",
    "    Xtr_2 = get_openai_embeddings(X_train, model=model_2, batch_size=64)\n",
    "    Xte_2 = get_openai_embeddings(X_test,  model=model_2, batch_size=64)\n",
    "    np.save(Xtr_path_2, Xtr_2)\n",
    "    np.save(Xte_path_2, Xte_2)\n",
    "\n",
    "Xtr_2.shape, Xte_2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af83d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI text-embedding-ada-002 + LR | acc=0.9250 prec=0.9101 rec=0.9444 f1=0.9270\n"
     ]
    }
   ],
   "source": [
    "clf_2 = LogisticRegression(max_iter=3000, random_state=42)\n",
    "clf_2.fit(Xtr_2, y_train)\n",
    "pred_2 = clf_2.predict(Xte_2)\n",
    "\n",
    "results.append(eval_binary(y_test, pred_2, name=f\"OpenAI {model_2} + LR\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39219068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OpenAI text-embedding-3-small + LR</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.930097</td>\n",
       "      <td>0.950397</td>\n",
       "      <td>0.940137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpenAI text-embedding-ada-002 + LR</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.910134</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.926972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                model  accuracy  precision    recall        f1\n",
       "0  OpenAI text-embedding-3-small + LR     0.939   0.930097  0.950397  0.940137\n",
       "1  OpenAI text-embedding-ada-002 + LR     0.925   0.910134  0.944444  0.926972"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.DataFrame(results).sort_values(\"f1\", ascending=False)\n",
    "res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9172f6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF (1,1) + LR | acc=0.8540 prec=0.8315 rec=0.8909 f1=0.8602\n",
      "TFIDF (1,2) + LR | acc=0.8560 prec=0.8371 rec=0.8869 f1=0.8613\n"
     ]
    }
   ],
   "source": [
    "tfidf_runs = [\n",
    "    (\"TFIDF (1,1) + LR\", (1,1)),\n",
    "    (\"TFIDF (1,2) + LR\", (1,2)),\n",
    "]\n",
    "\n",
    "for name, ngr in tfidf_runs:\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            lowercase=True,\n",
    "            stop_words=\"english\",\n",
    "            ngram_range=ngr,\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )),\n",
    "        (\"clf\", LogisticRegression(max_iter=3000, random_state=42))\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "    results.append(eval_binary(y_test, pred, name=name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae4b3f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF (1,2) + SVD100 + LR | acc=0.8310 prec=0.8454 rec=0.8135 f1=0.8291\n",
      "TFIDF (1,2) + SVD200 + LR | acc=0.8500 prec=0.8569 rec=0.8433 f1=0.8500\n",
      "TFIDF (1,2) + SVD300 + LR | acc=0.8600 prec=0.8655 rec=0.8552 f1=0.8603\n"
     ]
    }
   ],
   "source": [
    "svd_runs = [\n",
    "    (\"TFIDF (1,2) + SVD100 + LR\", 100),\n",
    "    (\"TFIDF (1,2) + SVD200 + LR\", 200),\n",
    "    (\"TFIDF (1,2) + SVD300 + LR\", 300),\n",
    "]\n",
    "\n",
    "for name, k in svd_runs:\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            lowercase=True,\n",
    "            stop_words=\"english\",\n",
    "            ngram_range=(1,2),\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )),\n",
    "        (\"svd\", TruncatedSVD(n_components=k, random_state=42)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=3000, random_state=42))\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "    results.append(eval_binary(y_test, pred, name=name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d05f3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_token_re = re.compile(r\"[a-z']+\")\n",
    "\n",
    "def tokenize_simple(text: str):\n",
    "    return _token_re.findall(text.lower())\n",
    "\n",
    "def avg_doc_vector(texts, kv):\n",
    "    X = np.zeros((len(texts), kv.vector_size), dtype=np.float32)\n",
    "    for i, t in enumerate(tqdm(texts, desc=\"Doc vectors\")):\n",
    "        toks = tokenize_simple(t)\n",
    "        vecs = [kv[w] for w in toks if w in kv]\n",
    "        if vecs:\n",
    "            X[i] = np.mean(vecs, axis=0)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "388d4ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec avg + LR | acc=0.8410 prec=0.8457 rec=0.8373 f1=0.8415\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"cache\", exist_ok=True)\n",
    "\n",
    "w2v_tr_path = \"cache/Xtr_word2vec.npy\"\n",
    "w2v_te_path = \"cache/Xte_word2vec.npy\"\n",
    "\n",
    "if os.path.exists(w2v_tr_path) and os.path.exists(w2v_te_path):\n",
    "    Xtr_w2v = np.load(w2v_tr_path)\n",
    "    Xte_w2v = np.load(w2v_te_path)\n",
    "else:\n",
    "    word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "    Xtr_w2v = avg_doc_vector(X_train, word2vec_model)\n",
    "    Xte_w2v = avg_doc_vector(X_test, word2vec_model)\n",
    "    np.save(w2v_tr_path, Xtr_w2v)\n",
    "    np.save(w2v_te_path, Xte_w2v)\n",
    "\n",
    "clf = LogisticRegression(max_iter=3000, random_state=42)\n",
    "clf.fit(Xtr_w2v, y_train)\n",
    "pred = clf.predict(Xte_w2v)\n",
    "results.append(eval_binary(y_test, pred, name=\"Word2Vec avg + LR\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25527c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe avg + LR | acc=0.8230 prec=0.8371 rec=0.8056 f1=0.8210\n"
     ]
    }
   ],
   "source": [
    "gl_tr_path = \"cache/Xtr_glove.npy\"\n",
    "gl_te_path = \"cache/Xte_glove.npy\"\n",
    "\n",
    "if os.path.exists(gl_tr_path) and os.path.exists(gl_te_path):\n",
    "    Xtr_gl = np.load(gl_tr_path)\n",
    "    Xte_gl = np.load(gl_te_path)\n",
    "else:\n",
    "    glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "    Xtr_gl = avg_doc_vector(X_train, glove_model)\n",
    "    Xte_gl = avg_doc_vector(X_test, glove_model)\n",
    "    np.save(gl_tr_path, Xtr_gl)\n",
    "    np.save(gl_te_path, Xte_gl)\n",
    "\n",
    "clf = LogisticRegression(max_iter=3000, random_state=42)\n",
    "clf.fit(Xtr_gl, y_train)\n",
    "pred = clf.predict(Xte_gl)\n",
    "results.append(eval_binary(y_test, pred, name=\"GloVe avg + LR\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d8faeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OpenAI text-embedding-3-small + LR</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.930097</td>\n",
       "      <td>0.950397</td>\n",
       "      <td>0.940137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpenAI text-embedding-ada-002 + LR</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.910134</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.926972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFIDF (1,2) + LR</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.837079</td>\n",
       "      <td>0.886905</td>\n",
       "      <td>0.861272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF (1,2) + SVD300 + LR</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.865462</td>\n",
       "      <td>0.855159</td>\n",
       "      <td>0.860279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TFIDF (1,1) + LR</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.831481</td>\n",
       "      <td>0.890873</td>\n",
       "      <td>0.860153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF (1,2) + SVD200 + LR</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.856855</td>\n",
       "      <td>0.843254</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Word2Vec avg + LR</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.845691</td>\n",
       "      <td>0.837302</td>\n",
       "      <td>0.841476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TFIDF (1,2) + SVD100 + LR</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.845361</td>\n",
       "      <td>0.813492</td>\n",
       "      <td>0.829120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GloVe avg + LR</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.837113</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.821031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                model  accuracy  precision    recall        f1\n",
       "0  OpenAI text-embedding-3-small + LR     0.939   0.930097  0.950397  0.940137\n",
       "1  OpenAI text-embedding-ada-002 + LR     0.925   0.910134  0.944444  0.926972\n",
       "3                    TFIDF (1,2) + LR     0.856   0.837079  0.886905  0.861272\n",
       "6           TFIDF (1,2) + SVD300 + LR     0.860   0.865462  0.855159  0.860279\n",
       "2                    TFIDF (1,1) + LR     0.854   0.831481  0.890873  0.860153\n",
       "5           TFIDF (1,2) + SVD200 + LR     0.850   0.856855  0.843254  0.850000\n",
       "7                   Word2Vec avg + LR     0.841   0.845691  0.837302  0.841476\n",
       "4           TFIDF (1,2) + SVD100 + LR     0.831   0.845361  0.813492  0.829120\n",
       "8                      GloVe avg + LR     0.823   0.837113  0.805556  0.821031"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.DataFrame(results).sort_values(\"f1\", ascending=False)\n",
    "res_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92774f",
   "metadata": {},
   "source": [
    "## Results and comparison\n",
    "\n",
    "We compared four representation families on the IMDB-5000 sentiment dataset using the same 80/20 stratified train/test split and a Logistic Regression classifier on top of each representation. Performance is reported with accuracy, precision, recall, and F1.\n",
    "\n",
    "### TF–IDF baselines (sparse bag-of-ngrams)\n",
    "TF–IDF with unigrams and bigrams produced similar performance (F1 ≈ 0.860–0.861). Adding bigrams gave only a marginal improvement over unigrams in this run.\n",
    "\n",
    "### TF–IDF + SVD (LSA)\n",
    "Applying TruncatedSVD to the TF–IDF matrix (LSA) did not improve results relative to the best TF–IDF baseline. SVD100 degraded performance (F1 ≈ 0.829), while SVD200–SVD300 recovered close to the baseline (F1 ≈ 0.850–0.860), but still did not exceed TF–IDF without dimensionality reduction.\n",
    "\n",
    "### Averaged pretrained word embeddings (dense, static)\n",
    "Document vectors formed by averaging pretrained word vectors performed worse than TF–IDF. Word2Vec averaging reached F1 ≈ 0.841, and GloVe averaging reached F1 ≈ 0.821. This is consistent with the limitation of simple averaging: it discards word order and can dilute sentiment-bearing phrases.\n",
    "\n",
    "### OpenAI document embeddings (dense, contextual)\n",
    "OpenAI embeddings substantially outperformed all other approaches. `text-embedding-3-small` achieved the best overall result (accuracy ≈ 0.939, F1 ≈ 0.940). The older `text-embedding-ada-002` was slightly weaker (accuracy ≈ 0.925, F1 ≈ 0.927). These embeddings likely capture higher-level semantic and sentiment cues beyond surface n-gram statistics.\n",
    "\n",
    "### Overall ranking (by F1 in this run)\n",
    "1. OpenAI `text-embedding-3-small` + LR (F1 ≈ 0.940)\n",
    "2. OpenAI `text-embedding-ada-002` + LR (F1 ≈ 0.927)\n",
    "3. TF–IDF (1,2) + LR / TF–IDF + SVD300 + LR (F1 ≈ 0.860–0.861)\n",
    "4. Word2Vec average + LR (F1 ≈ 0.841)\n",
    "5. GloVe average + LR (F1 ≈ 0.821)\n",
    "\n",
    "## Implementation notes\n",
    "\n",
    "OpenAI embeddings were computed in batches and cached to disk to avoid repeated API calls. Word2Vec and GloVe document embeddings were constructed by averaging in-vocabulary word vectors for each review and then training a classifier on these dense document vectors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
